{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Downloading text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://www.gutenberg.org/files/11/11-0.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\matni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\matni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\matni\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove non-alphabetic characters and numbers\n",
    "    text = re.sub(r'[^a-z ]', '', text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Most important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_names = text.split('CHAPTER')[1:13]\n",
    "chapters = text.split('CHAPTER')[13:]\n",
    "chapter_names_processed = [process_text(name) for name in chapter_names]\n",
    "chapters_processed = [process_text(_) for _ in chapters]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Find top 10 words for each chapter\n",
    "top_words_by_chapter = []\n",
    "for chapter in chapters_processed:\n",
    "    # Calculate TF-IDF scores for words in the chapter\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([chapter])\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    top_words = [feature_names[i] for i in tfidf_matrix.sum(axis=0).argsort()[0, -10:][::-1]]\n",
    "    top_words_by_chapter.append(top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rabbithole: nothing, door, thought, think, way, one, see, like, little, alice\n",
      "ii pool tear: one, must, went, foot, dear, thing, said, mouse, little, alice\n",
      "iii caucusrace long tale: lory, one, soon, long, thing, know, dodo, mouse, alice, said\n",
      "iv rabbit sends little bill: get, bill, thought, heard, quite, one, rabbit, said, little, alice\n",
      "v advice caterpillar: dont, serpent, ive, size, pigeon, im, little, caterpillar, alice, said\n",
      "vi pig pepper: baby, went, little, much, footman, duchess, like, cat, alice, said\n",
      "vii mad teaparty: went, thing, know, time, hare, march, dormouse, hatter, alice, said\n",
      "viii queen croquetground: went, three, two, see, cat, king, head, alice, queen, said\n",
      "ix mock turtle story: say, dont, queen, went, gryphon, duchess, turtle, mock, alice, said\n",
      "x lobster quadrille: join, lobster, beautiful, wont, would, alice, gryphon, turtle, mock, said\n",
      "xi stole tart: thought, court, rabbit, dormouse, queen, one, alice, hatter, king, said\n",
      "xii alices evidence: state, copy, term, electronic, gutenberg, foundation, gutenbergtm, said, work, project\n"
     ]
    }
   ],
   "source": [
    "for i, top_words in enumerate(top_words_by_chapter):\n",
    "    chapter_name = ', '.join(top_words[0][0])\n",
    "    print(f\"{chapter_names_processed[i]}: {chapter_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rabbithole Chronicles: Through the Door of Thought\n",
    "\n",
    "The Pool of Tears: Footprints of the Dear Mouse\n",
    "\n",
    "The Caucus Race and the Long Tale: Lory's Knowing Dodo\n",
    "\n",
    "Rabbit Sends Little Bill: Chasing the White Rabbit's Thoughts\n",
    "\n",
    "Advice from the Caterpillar: Conversations with the Wise Serpent\n",
    "\n",
    "Pig and Pepper: Of Babies and Cheshire Cats\n",
    "\n",
    "The Mad Teaparty: Hares, March Hares, and Dormice\n",
    "\n",
    "The Queen's Croquet Ground: Heads, Cats, and Royal Decrees\n",
    "\n",
    "The Mock Turtle's Story: Gryphons and Mock Turtles Speak\n",
    "\n",
    "The Lobster Quadrille: Of Beautiful Lobsters and Unlikely Dances\n",
    "\n",
    "Who Stole the Tart: Trials and Witnesses in Wonderland\n",
    "\n",
    "Alice's Evidence: A State of Electronic Wonderland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Most often words with Alice word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\matni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\matni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most used verbs with Alice:\n",
      "[('said', 166), ('thought', 35), ('went', 27), ('say', 20), ('looked', 18), ('began', 18), ('got', 16), ('know', 16), ('see', 15), ('think', 14)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize sentences\n",
    "sentences = re.split(r'[.!?]', text)\n",
    "sentences = [process_text(_) for _ in sentences]\n",
    "alice_verbs = []\n",
    "\n",
    "# Extract verbs in sentences with \"Alice\"\n",
    "for sentence in sentences:\n",
    "    if 'alice' in sentence:\n",
    "        words = word_tokenize(sentence)\n",
    "        # Perform part-of-speech tagging\n",
    "        tagged_words = pos_tag(words)\n",
    "        # Extract verbs (VB*)\n",
    "        verbs = [word for word, pos in tagged_words if pos.startswith('VB')]\n",
    "        alice_verbs.extend(verbs)\n",
    "\n",
    "# Count verb occurrences\n",
    "verb_counts = Counter(alice_verbs)\n",
    "\n",
    "# Print top 10 most used verbs with Alice\n",
    "print(\"Top 10 most used verbs with Alice:\")\n",
    "print(verb_counts.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most often Alice says and then thinks :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
